---
title: "LMN: Inference for Linear Models with Nuisance Parameters"
author: "Martin Lysy, Bryan Yates"
date: "`r Sys.Date()`"
output:
  #bookdown::html_document2:
  #  base_format: rmarkdown::html_vignette
  rmarkdown::html_vignette:
    toc: true
bibliography: references.bib
csl: taylor-and-francis-harvard-x.csl
link-citations: true
vignette: >
  %\VignetteIndexEntry{Getting Started with LMN}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<!-- equation numbering -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

```{r setup, include = FALSE}
knitr::opts_chunk$set(out.width = "\\textwidth")
suppressMessages({
  require(LMN)
  require(kableExtra)
})
cran_link <- function(pkg) paste0("[**", pkg, "**](https://CRAN.R-project.org/package=", pkg, ")")
```

\newcommand{\YY}{{\boldsymbol{Y}}}
\newcommand{\XX}{{\boldsymbol{X}}}
\newcommand{\VV}{{\boldsymbol{V}}}
\newcommand{\ZZ}{{\boldsymbol{Z}}}
\newcommand{\SS}{{\boldsymbol{S}}}
\newcommand{\TT}{{\boldsymbol{T}}}
\newcommand{\EE}{{\boldsymbol{E}}}
\newcommand{\bbe}{{\boldsymbol{\beta}}}
\newcommand{\BBe}{{\boldsymbol{B}}}
\newcommand{\SSi}{{\boldsymbol{\Sigma}}}
\newcommand{\tth}{{\boldsymbol{\theta}}}
\newcommand{\LLa}{{\boldsymbol{\Lambda}}}
\newcommand{\OOm}{{\boldsymbol{\Omega}}}
\newcommand{\TTh}{{\boldsymbol{\Theta}}}
\newcommand{\mmu}{{\boldsymbol{\mu}}}
\newcommand{\eet}{{\boldsymbol{\eta}}}
\newcommand{\MN}{\textrm{Matrix-Normal}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\bz}{\boldsymbol{0}}
\renewcommand{\vec}{\textrm{vec}}
\newcommand{\ud}{\mathrm{d}}
\newcommand{\eps}{\varepsilon}
\newcommand{\a}{\alpha}
\newcommand{\g}{\gamma}
\newcommand{\s}{\sigma}
\newcommand{\l}{\lambda}
\newcommand{\dt}{\Delta t}
\newcommand{\var}{\mathrm{var}}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\iid}{\stackrel {\textrm{iid}}{\sim}}
\newcommand{\ind}{\stackrel {\textrm{ind}}{\sim}}
\newcommand{\llp}{\ell_{\textrm{prof}}}
\newcommand{\diag}{\textrm{diag}}

## Introduction

Consider a statistical model $p(\YY \mid \tth, \BBe, \SSi)$ of the form
\begin{equation}
\YY \sim \MN(\XX_\tth \BBe, \VV_\tth, \SSi),
\label{eq:lmn}
\end{equation}
$\XX_\tth = \XX_{n \times p}(\tth)$ is the design matrix which depends on parameters $\tth$, $\BBe_{p \times q}$ are regression coefficients, $\VV_\tth = \VV_{n \times n}(\tth)$ and $\SSi_{q \times q}$ are between-row and between-column variance matrices, and the [Matrix-Normal](https://en.wikipedia.org/wiki/Matrix_normal_distribution) distribution is defined as
$$
\ZZ_{p \times q} \sim \MN(\LLa_{p \times q}, \OOm_{p \times p}, \SSi_{q \times q}) \quad \iff \quad \vec(\ZZ) \sim \N(\vec(\LLa), \SSi \otimes \OOm),
$$
where $\vec(\ZZ)$ is a vector stacks the columns of $\ZZ$, and $\SSi \otimes \OOm$ denotes the [Kronecker product](https://en.wikipedia.org/wiki/Kronecker_product).

Model \\eqref{eq:lmn} is referred to as a Linear Model with Nuisance parameters (LMN) $(\BBe, \SSi)$ for parameters of interest $\tth$.  The **LMN** package provides tools to efficiently conduct Frequentist or Bayesian inference on all parameters $\TTh = (\tth, \BBe, \SSi)$ by estimating $\tth$ first, and subsequently $(\BBe, \SSi)$, as illustrated in the examples below.

<!-- The **LMN** package provides several functions to facilitate inference with multivariate regression models of the form -->
<!-- $$ -->
<!-- \YY \sim \MN(\XX_\tth \BBe, \VV_\tth, \SSi), -->
<!-- $$ -->
<!-- where $\YY_{n\times q}$ is the response matrix, $\XX_\tth = \XX_{n \times p}(\tth)$ is the design matrix which depends on parameters $\tth$, $\VV_\tth = \VV_{n \times n}(\tth)$ and $\SSi_{q \times q}$ are between-row and between-column variance matrices, $\BBe_{p \times q}$ are regression coefficients, and $\MN(\LLa, \OOm, \SSi)$ denotes the Matrix-Normal distribution: -->
<!-- $$ -->
<!-- \ZZ_{p \times q} \sim \MN(\LLa_{p \times q}, \OOm_{p \times p}, \SSi_{q \times q}) \quad \iff \quad \vec(\ZZ) \sim \N(\vec(\LLa), \SSi \otimes \OOm). -->
<!-- $$ -->
<!-- The parameters of the regression model are $\TTh = (\tth, \BBe, \SSi)$. -->

## Example 1: Nonlinear Regression with Correlated Errors

```{r toy_sim_calc, include = FALSE}
require(LMN)

# problem dimensions
qq <- 2 # number of responses
n <- 200 # number of observations

# parameters
Beta <- matrix(c(.3, .5, .7, .2), 2, qq)
Sigma <- matrix(c(.005, -.001, -.001, .002), qq, qq)
alpha <- .4
lambda <- .1

# simulate data
xseq <- seq(0, 10, len = n) # x vector
X <- cbind(1, xseq^alpha) # covariate matrix
V <- exp(-(outer(xseq, xseq, "-")/lambda)^2) # between-row variance
Mu <- X %*% Beta # mean matrix
## Y <- mniw::rMNorm(n = 1, Mu = Mu, RowV = V, ColV = Sigma) # response matrix
# response matrix
Y <- matrix(rnorm(n*qq), n, qq)
Y <- crossprod(chol(V), Y) %*% chol(Sigma) + Mu

# plot data
par(mfrow = c(1,1), mar = c(4,4,.5,.5)+.1)
plot(x = 0, type = "n", xlim = range(xseq), ylim = range(Mu, Y),
     xlab = "x", ylab = "y")
lines(x = xseq, y = Mu[,1], col = "red")
lines(x = xseq, y = Mu[,2], col = "blue")
points(x = xseq, y = Y[,1], pch = 16, col = "red")
points(x = xseq, y = Y[,2], pch = 16, col = "blue")
legend("bottomright",
       legend = c("Response 1", "Response 2", "Expected", "Observed"),
       lty = c(NA, NA, 1, NA), pch = c(22, 22, NA, 16),
       pt.bg = c("red", "blue", "black", "black"), seg.len = 1)
```

Consider a toy example of the form
\begin{equation}
y_{ij} = \beta_{0j} + \beta_{1j} x_i^\alpha + \eps_{ij},
\label{eq:toy}
\end{equation}
where $i=1,\ldots,n$, $j=1,\ldots,q$, and the $\eps_{ij}$ are multivariate normal with $E[\eps_{ij}] = 0$ and
$$
\cov(\eps_{ik}, \eps_{jm}) = \Sigma_{ij} \times \exp\left\{-\frac{(x_k - x_m)^2}{\lambda^2}\right\}.
$$
Then this toy example can be written in the form of \\eqref{eq:lmn} with $\tth = (\alpha, \lambda)$ and
<!-- , $\YY_{n\times q} = [y_{ij}]$, $\VV_{n\times n}(\tth) = [V_{ij}(\lambda)]$ where $V_{ij}(\lambda) = \exp\{-(x_i-x_j)^2/\lambda^2\}$, $\Sigma_{q \times q} = [\Sigma_{ij}]$, and -->
$$
\begin{aligned}
\YY_{n\times q} & = \begin{bmatrix} y_{11} & \cdots & y_{1q} \\ \vdots & & \vdots \\ y_{n1} & \cdots & y_{nq} \end{bmatrix}, 
& 
\VV_{n\times n}(\tth) & = \begin{bmatrix} e^{-\frac{(x_1-x_1)^2}{\lambda^2}} & \cdots & e^{-\frac{(x_1-x_n)^2}{\lambda^2}} \\ \vdots & & \vdots \\ e^{-\frac{(x_n-x_1)^2}{\lambda^2}} & \cdots & e^{-\frac{(x_n-x_n)^2}{\lambda^2}} \end{bmatrix},
&
\XX_{n\times 2}(\tth) & = \begin{bmatrix} 1 & x_1^\alpha \\ \vdots & \vdots \\ 1 & x_n^\alpha \end{bmatrix}, 
\\
\BBe_{2\times q} & = \begin{bmatrix} \beta_{01} & \cdots & \beta_{0q} \\ \beta_{11} & \cdots & \beta_{1q} \end{bmatrix}, 
& 
\SSi_{q\times q} & = \begin{bmatrix} \Sigma_{11} & \cdots & \Sigma_{1q} \\ \vdots & & \vdots \\ \Sigma_{q1} & \cdots & \Sigma_{qq} \end{bmatrix}. 
\end{aligned}
$$
Sample data is generated below with $n = `r n`$ and $q = `r qq`$.
```{r toy_sim, ref.label = "toy_sim_calc", fig.height = 4, fig.width = 6.5, fig.cap = paste0("Simulated data from the toy model \\\\eqref{eq:toy} with $n = ", n, "$ and $q = ", qq, "$.")}
```
The workhorse function in **LMN** is `lmn.suff()`, which calculates the sufficient statistics for $(\BBe, \SSi)$ for a given value of $\tth$.
```{r}
suff <- lmn.suff(Y = Y, X = X, V = V, Vtype = "full")
sapply(suff, function(x) paste0(dim(as.array(x)), collapse = "x"))
```
Namely, the elements of `suff` are:

  - `Bhat`: The $p \times q$ matrix $\hat \BBe_\tth = (\XX_\tth'\VV_\tth^{-1}\XX_\tth)^{-1}\XX_\tth'\VV_\tth^{-1}\YY$.
  - `T`: The $p \times p$ matrix $\TT_\tth = \XX_\tth'\VV_\tth^{-1}\XX_\tth$.
  - `S`: The $q \times q$ matrix $\SS_\tth = (\YY - \XX_\tth\hat\BBe_\tth)'\VV_\tth^{-1}(\YY - \XX_\tth\hat\BBe_\tth)$.
  - `ldV`: The log-determinant $\log |\VV_\tth|$.
  - `n`, `p`, `q`: The dimensions of the problem.
  
In particular, the MLEs of $(\BBe, \SSi)$ given a particular value of $\tth$ are $\hat \BBe_\tth$ and $\hat \SSi_\tth = \SS_\tth/n$.

### Profile Likelihood

The profile (log)likelihood function for the LMN model corresponds to evaluating the full likelihood as a function of $\tth$ at the conditional MLE of $(\BBe, \SSi)$:
$$
\llp(\tth \mid \YY) = \ell_{\textrm{full}}(\tth, \BBe = \hat \BBe_\tth, \SSi = \hat \SSi_\tth \mid \YY).
$$
The following R code shows how to calculate the full MLE $\hat \TTh = (\hat \tth, \hat \BBe, \hat \SSi)$ from only a 2D optimization of $\llp(\alpha, \lambda \mid \YY)$.  The first step is to now that for equally-spaced $x_i$ as in the simulated data above, the between-row variance matrix $\VV_\tth$ is in fact [Toeplitz](https://en.wikipedia.org/wiki/Toeplitz_matrix).  It is entirely determined by its first row, and can be inverted much more efficiently than dense matrices using the R package `r cran_link("SuperGauss")`.
```{r, results = "hold"}
# check than dense matrix and Toeplitz matrix calculations are the same

# autocorrelation function, or first row of V_theta
toy_acf <- function(lambda) exp(-((xseq-xseq[1])/lambda)^2)

# check that calculation of suff is the same
all.equal(suff,
          lmn.suff(Y = Y, X = X, V = toy_acf(lambda), Vtype = "acf"))

# check that it's much faster to use Vtype = "acf"
system.time({
  # using dense variance matrix
  replicate(100, lmn.suff(Y = Y, X = X, V = V, Vtype = "full"))
})
system.time({
  # using Toeplitz variance matrix
  replicate(100, lmn.suff(Y = Y, X = X, V = toy_acf(lambda), Vtype = "acf"))
})
```
Next, let's find the value of the MLE $\hat \TTh$ using `stats::optim()`.  For display purposes the parameters will be written as $\TTh = (\alpha, \lambda, \beta_{01}, \beta_{02}, \beta_{11}, \beta_{12}, \sigma_1 = \Sigma_{11}^{1/2}, \sigma_2 = \Sigma_{22}^{1/2}, \rho_{12} = \Sigma_{12}/(\sigma_1\sigma_2))$.  Also, as the **SuperGauss** computations require some memory allocation, we'll declare a `SuperGauss::Toeplitz()` object once and re-use it within the optimization routine.
```{r}
# pre-allocate memory for Toeplitz matrix calcuations
Tz <- SuperGauss::Toeplitz(n = n)

# sufficient statistics for the toy model
# sufficient statistics for toy model
toy_suff <- function(theta) {
  X <- cbind(1, xseq^theta[1])
  Tz$setAcf(acf = toy_acf(theta[2]))
  lmn.suff(Y = Y, X = X, V = Tz, Vtype = "acf")
}

# _negative_ profile likelihood for theta
toy_prof <- function(theta) {
  if(theta[2] < 0) return(-Inf) # range restriction lambda > 0
  suff <- toy_suff(theta)
  -lmn.prof(suff = suff)
}

# MLE of theta
opt <- optim(par = c(alpha, lambda), # starting value
             fn = toy_prof) # objective function
theta_mle <- opt$par

# MLE of (Beta, Sigma)
suff <- toy_suff(theta_mle)
Beta_mle <- suff$Bhat
Sigma_mle <- suff$S/suff$n

# display:
# convert variance matrix to vector of standard deviations and correlations.
cov2sigrho <- function(Sigma) {
  sig <- sqrt(diag(Sigma))
  n <- length(sig) # dimensions of Sigma
  names(sig) <- paste0("sigma",1:n)
  # indices of upper triangular elements
  iupper <- matrix(1:(n^2),n,n)[upper.tri(Sigma, diag = FALSE)]
  rho <- cov2cor(Sigma)[iupper]
  rnames <- apply(expand.grid(1:n, 1:n), 1, paste0, collapse = "")
  names(rho) <- paste0("rho",rnames[iupper])
  c(sig,rho)
}

Theta_mle <- c(theta_mle, t(Beta_mle), cov2sigrho(Sigma_mle))
names(Theta_mle) <- c("alpha", "lambda",
                      "beta_01", "beta_02", "beta_11", "beta_12",
                      "sigma_1", "sigma_2", "rho_12")
signif(Theta_mle, 2)
```
Finally, let's calculate standard errors for each parameter using $\sqrt{\diag(\widehat{\var}(\hat \TTh))}$, where
$$
\widehat{\var}(\hat \TTh) = -\left[\frac{\partial^2 \ell(\TTh \mid \YY)}{\partial \TTh\partial\TTh'} \right]^{-1}.
$$
This can be done numerically using the R package `r cran_link("numDeriv")` function `numDeriv::hessian()`:
```{r}
# full _negative_ loglikelihood for the toy model
toy_nll <- function(Theta) {
  theta <- Theta[1:2]
  Beta <- rbind(Theta[3:4], Theta[5:6])
  Sigma <- diag(Theta[7:8]^2)
  Sigma[1,2] <- Sigma[2,1] <- Theta[7]*Theta[8]*Theta[9]
  # calculate loglikelihood
  suff <- toy_suff(theta)
  -lmn.loglik(Beta = Beta, Sigma = Sigma, suff = suff)
}

# uncertainty estimate:
# variance estimator
Theta_ve <- solve(numDeriv::hessian(func = toy_nll, x = Theta_mle))
Theta_se <- sqrt(diag(Theta_ve)) # standard errors

# display
tab <- rbind(true = c(alpha, lambda, t(Beta), cov2sigrho(Sigma)),
             mle = Theta_mle, se = Theta_se)
colnames(tab) <- paste0("$\\", gsub("([0-9]+)", "{\\1}", names(Theta_mle)), "$")
rownames(tab) <- c("True Value", "MLE", "Std. Error")
kableExtra::kable(as.data.frame(signif(tab,2)))
```

## Example 2: Generalized Cox-Ingersoll-Ross Process

In @chan.etal92, a model for the interest rate $X_t$ as a function of time is given by the stochastic differential equation (SDE)
\begin{equation}
\ud X_t = -\g(X_t - \mu) \ud t + \s X_t^\l\ud B_t,
\label{eq:chan}
\end{equation}
where $B_t$ is Brownian motion.  Suppose the data $\XX = (X_0, \ldots, X_N)$ consists of equispaced observations $X_n = X_{n\cdot \dt}$ with interobservation time $\dt$.  A commonly-used discrete-time approximation is given by
\begin{equation}
X_{n+1} \mid X_0,\ldots,X_n \sim \N\big(X_n - \g(X_n - \mu)\dt, \s^2 X_n^{2\l} \dt\big).
\label{eq:euler}
\end{equation}
Sample data is generated below:

### Bayesian Inference

## References


<!-- ## Example 1: Fractional Brownian Motion with Drift -->

<!-- In @lysy.etal16, a model for subdiffusive microparticles in viscoelastic fluids is given by -->
<!-- $$ -->
<!-- \XX_t = \mmu t + \eet t^2 + \SSi^{1/2} \ZZ_t, -->
<!-- $$ -->
<!-- where $\XX_t = (X_{1t}, X_{2t}, X_{3t})$ is the position of the microparticle at time $t$, \mmu t + \eet t^2 = \sum_{j=1}^3 \mu_j t + \eta_j t^2$ is a quadratic drift term, $\SSi_{3 \times 3}$ is a symmetric positive-definite scale matrix, and $\ZZ_t = (Z_{1t}, Z_{2t}, Z_{3t})$ are iid fractional Brownian motion (fBM) processes, i.e., continuous mean-zero Gaussian processes with -->
<!-- $$ -->
<!-- \cov(Z_{js}, Z_{jt}) = \tfrac 1 2 (|t|^\a + |s|^\a - |t-s|^\a), \qquad 0 < \a < 2. -->
<!-- $$ -->
